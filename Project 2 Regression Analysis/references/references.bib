
@misc{paul_project_2023,
	title = {Project 2: {Regression} {Analysis}},
	url = {https://canvas.brown.edu/courses/1092384/assignments/7961814},
	urldate = {2023-11-06},
	author = {Paul, Alice},
	month = nov,
	year = {2023},
	file = {Project 2\: Regression Analysis:/Users/shirleysong/Zotero/storage/WKWLHTJ9/7961814.html:text/html},
}

@misc{mckinney_predicting_2023,
	title = {Predicting the need for tracheostomy in infants with severe bronchopulmonary dysplasia},
	author = {McKinney, Robin and Levin, Jon},
	month = oct,
	year = {2023},
	note = {Published: Presented at Brown Alpert Medical School},
	annote = {Lecture slides},
}

@article{rubin_inference_1976,
	title = {Inference and missing data},
	volume = {63},
	number = {3},
	journal = {Biometrika},
	author = {Rubin, Donald B},
	year = {1976},
	note = {Publisher: Oxford University Press},
	pages = {581--592},
}

@incollection{rubin_multiple_2018,
	title = {Multiple imputation},
	booktitle = {Flexible {Imputation} of {Missing} {Data}, {Second} {Edition}},
	publisher = {Chapman and Hall/CRC},
	author = {Rubin, Donald B},
	year = {2018},
	pages = {29--62},
}

@article{baraldi_introduction_2010,
	title = {An introduction to modern missing data analyses},
	volume = {48},
	number = {1},
	journal = {Journal of school psychology},
	author = {Baraldi, Amanda N and Enders, Craig K},
	year = {2010},
	note = {Publisher: Elsevier},
	pages = {5--37},
}

@article{sjoberg_reproducible_2021,
	title = {Reproducible {Summary} {Tables} with the gtsummary {Package}},
	volume = {13},
	url = {https://doi.org/10.32614/RJ-2021-053},
	doi = {10.32614/RJ-2021-053},
	number = {1},
	journal = {The R Journal},
	author = {Sjoberg, Daniel D. and Whiting, Karissa and Curry, Michael and Lavery, Jessica A. and Larmarange, Joseph},
	year = {2021},
	pages = {570--580},
}

@article{buuren_mice_2011,
	title = {mice: {Multivariate} {Imputation} by {Chained} {Equations} in {R}},
	volume = {45},
	doi = {10.18637/jss.v045.i03},
	number = {3},
	journal = {Journal of Statistical Software},
	author = {Buuren, Stef van and Groothuis-Oudshoorn, Karin},
	year = {2011},
	pages = {1--67},
}

@article{hazimeh_l0learn_2023,
	title = {L0learn: {A} scalable package for sparse learning using l0 regularization},
	volume = {24},
	number = {205},
	journal = {Journal of Machine Learning Research},
	author = {Hazimeh, Hussein and Mazumder, Rahul and Nonet, Tim},
	year = {2023},
	pages = {1--8},
}

@article{dedieu_learning_2021,
	title = {Learning {Sparse} {Classifiers}: {Continuous} and {Mixed} {Integer} {Optimization} {Perspectives}},
	volume = {22},
	url = {http://jmlr.org/papers/v22/19-1049.html},
	number = {135},
	journal = {Journal of Machine Learning Research},
	author = {Dedieu, Antoine and Hazimeh, Hussein and Mazumder, Rahul},
	year = {2021},
	pages = {1--47},
}

@article{bertsimas_best_2016,
	title = {Best subset selection via a modern optimization lens},
	volume = {44},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-44/issue-2/Best-subset-selection-via-a-modern-optimization-lens/10.1214/15-AOS1388.full},
	doi = {10.1214/15-AOS1388},
	abstract = {In the period 1991–2015, algorithmic advances in Mixed Integer Optimization (MIO) coupled with hardware improvements have resulted in an astonishing 450 billion factor speedup in solving MIO problems. We present a MIO approach for solving the classical best subset selection problem of choosing \$k\$ out of \$p\$ features in linear regression given \$n\$ observations. We develop a discrete extension of modern first-order continuous optimization methods to find high quality feasible solutions that we use as warm starts to a MIO solver that finds provably optimal solutions. The resulting algorithm (a) provides a solution with a guarantee on its suboptimality even if we terminate the algorithm early, (b) can accommodate side constraints on the coefficients of the linear regression and (c) extends to finding best subset solutions for the least absolute deviation loss function. Using a wide variety of synthetic and real datasets, we demonstrate that our approach solves problems with \$n\$ in the 1000s and \$p\$ in the 100s in minutes to provable optimality, and finds near optimal solutions for \$n\$ in the 100s and \$p\$ in the 1000s in minutes. We also establish via numerical experiments that the MIO approach performs better than Lasso and other popularly used sparse learning procedures, in terms of achieving sparse solutions with good predictive power.},
	number = {2},
	urldate = {2023-11-11},
	journal = {The Annals of Statistics},
	author = {Bertsimas, Dimitris and King, Angela and Mazumder, Rahul},
	month = apr,
	year = {2016},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Lasso, \${\textbackslash}ell\_\{0\}\$-constrained minimization, 62G35, 62J05, 62J07, 90C11, 90C26, 90C27, algorithms, Best subset selection, discrete optimization, global optimization, least absolute deviation, mixed integer programming, sparse linear regression},
	pages = {813--852},
	file = {Full Text PDF:/Users/shirleysong/Zotero/storage/WRUAACJN/Bertsimas et al. - 2016 - Best subset selection via a modern optimization le.pdf:application/pdf},
}

@article{beale_discarding_1967,
	title = {The {Discarding} of {Variables} in {Multivariate} {Analysis}},
	volume = {54},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2335028},
	doi = {10.2307/2335028},
	abstract = {In many multivariate situations we are presented with more variables than we would like, and the question arises whether they are all necessary and if not which can be discarded. In this paper we consider two such situations. (a) Regression analysis. The problem here is whether any variables can be discarded as adding little or nothing to the accuracy with which the regression equation correlates with the dependent variable. (b) Interdependence analysis. The problem is whether a constellation in p dimensions collapses, exactly or approximately, into fewer dimensions, and if so whether any of the original variables can be discarded. We may define the best solution to (a) using any given number of variables as the one that maximizes the multiple correlation between the selected variables and the dependent variable, and similarly for (b) as the one that maximizes the smallest multiple correlation with any of the rejected variables. In practice it is usual to accept an approximate solution to (a) based on `step-wise' multiple regression: we know of no standard program for (b). We have developed cut-off rules that enable us to find the best solution to both problems by partial enumeration. The paper discusses the details of this approach, and computational experience.},
	number = {3/4},
	urldate = {2023-11-11},
	journal = {Biometrika},
	author = {Beale, E. M. L. and Kendall, M. G. and Mann, D. W.},
	year = {1967},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {357--366},
	file = {JSTOR Full Text PDF:/Users/shirleysong/Zotero/storage/GCGJMGNN/Beale et al. - 1967 - The Discarding of Variables in Multivariate Analys.pdf:application/pdf},
}

@article{tibshirani_regression_1996,
	title = {Regression {Shrinkage} and {Selection} via the {Lasso}},
	volume = {58},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2346178},
	abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
	number = {1},
	urldate = {2023-11-11},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Tibshirani, Robert},
	year = {1996},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {267--288},
	file = {JSTOR Full Text PDF:/Users/shirleysong/Zotero/storage/XJU9JBJT/Tibshirani - 1996 - Regression Shrinkage and Selection via the Lasso.pdf:application/pdf},
}

@article{wood_how_2008,
	title = {How should variable selection be performed with multiply imputed data?},
	volume = {27},
	copyright = {Copyright © 2008 John Wiley \& Sons, Ltd.},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.3177},
	doi = {10.1002/sim.3177},
	abstract = {Multiple imputation is a popular technique for analysing incomplete data. Given the imputed data and a particular model, Rubin's rules (RR) for estimating parameters and standard errors are well established. However, there are currently no guidelines for variable selection in multiply imputed data sets. The usual practice is to perform variable selection amongst the complete cases, a simple but inefficient and potentially biased procedure. Alternatively, variable selection can be performed by repeated use of RR, which is more computationally demanding. An approximation can be obtained by a simple ‘stacked’ method that combines the multiply imputed data sets into one and uses a weighting scheme to account for the fraction of missing data in each covariate. We compare these and other approaches using simulations based around a trial in community psychiatry. Most methods improve on the naïve complete-case analysis for variable selection, but importantly the type 1 error is only preserved if selection is based on RR, which is our recommended approach. Copyright © 2008 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {17},
	urldate = {2023-11-11},
	journal = {Statistics in Medicine},
	author = {Wood, Angela M. and White, Ian R. and Royston, Patrick},
	year = {2008},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.3177},
	keywords = {multiple imputation, multiply imputed data, stacked data, stepwise, variable selection},
	pages = {3227--3246},
	file = {Full Text PDF:/Users/shirleysong/Zotero/storage/BJ9JTG9N/Wood et al. - 2008 - How should variable selection be performed with mu.pdf:application/pdf;Snapshot:/Users/shirleysong/Zotero/storage/WDQPE6UJ/sim.html:text/html},
}

@misc{du_variable_2020,
	title = {Variable selection with multiply-imputed datasets: choosing between stacked and grouped methods},
	shorttitle = {Variable selection with multiply-imputed datasets},
	url = {http://arxiv.org/abs/2003.07398},
	doi = {10.48550/arXiv.2003.07398},
	abstract = {Penalized regression methods, such as lasso and elastic net, are used in many biomedical applications when simultaneous regression coefficient estimation and variable selection is desired. However, missing data complicates the implementation of these methods, particularly when missingness is handled using multiple imputation. Applying a variable selection algorithm on each imputed dataset will likely lead to different sets of selected predictors, making it difficult to ascertain a final active set without resorting to ad hoc combination rules. In this paper we consider a general class of penalized objective functions which, by construction, force selection of the same variables across multiply-imputed datasets. By pooling objective functions across imputations, optimization is then performed jointly over all imputed datasets rather than separately for each dataset. We consider two objective function formulations that exist in the literature, which we will refer to as "stacked" and "grouped" objective functions. Building on existing work, we (a) derive and implement efficient cyclic coordinate descent and majorization-minimization optimization algorithms for both continuous and binary outcome data, (b) incorporate adaptive shrinkage penalties, (c) compare these methods through simulation, and (d) develop an R package miselect for easy implementation. Simulations demonstrate that the "stacked" objective function approaches tend to be more computationally efficient and have better estimation and selection properties. We apply these methods to data from the University of Michigan ALS Patients Repository (UMAPR) which aims to identify the association between persistent organic pollutants and ALS risk.},
	urldate = {2023-11-11},
	publisher = {arXiv},
	author = {Du, Jiacong and Boss, Jonathan and Han, Peisong and Beesley, Lauren J. and Goutman, Stephen A. and Batterman, Stuart and Feldman, Eva L. and Mukherjee, Bhramar},
	month = mar,
	year = {2020},
	note = {arXiv:2003.07398 [stat]},
	keywords = {Statistics - Methodology, Statistics - Applications, Statistics - Computation},
	annote = {Comment: 23 pages, 6 figures. This paper has been submitted to Statistics in Medicine},
	file = {arXiv Fulltext PDF:/Users/shirleysong/Zotero/storage/DGF2SPXS/Du et al. - 2020 - Variable selection with multiply-imputed datasets.pdf:application/pdf;arXiv.org Snapshot:/Users/shirleysong/Zotero/storage/NN2JQ2HP/2003.html:text/html},
}

@misc{paul_health_2023,
	title = {Health {Data} {Science} in {R}},
	url = {https://alicepaul.github.io/health-data-science-in-r/book/11_logistic_regression.html},
	urldate = {2023-11-11},
	author = {Paul, Alice},
	month = aug,
	year = {2023},
	file = {11. Logistic Regression — R for Health Data Science:/Users/shirleysong/Zotero/storage/JIUJR4SA/11_logistic_regression.html:text/html},
}

@article{zhang_improved_2018,
	title = {Improved {Growth} and {Infant} {Participation} in {Developmental} {Activities} {After} {Tracheostomy} {Placement} in {Infants} with {Severe} {Bronchopulmonary} {Dysplasia}},
	volume = {141},
	issn = {0031-4005},
	url = {https://doi.org/10.1542/peds.141.1MA6.530},
	doi = {10.1542/peds.141.1MA6.530},
	abstract = {Background: Very low birth weight (VLBW; BW ≤1500g) infants with severe BPD may require prolonged mechanical ventilation. While tracheostomy in these patients is associated with adverse neurodevelopmental outcomes, causality is difficult to show and tracheostomy placement may have other benefits in addition to provide a stable airway in this population. Objective: To evaluate growth indices and participation in developmental activities before and after tracheostomy in VLBW infants with severe BPD. Method: We conducted a retrospective analysis of VLBW infants born at ≤32 week gestation who were followed by the Newborn and Infant Chronic Lung Disease Program at the Children’s Hospital of Philadelphia and underwent tracheostomy between January 2009 and December 2014 due to sBPD (NIH consensus definition). We collected weight, height, and head circumference at 4 weeks before, at the time of and 4 weeks after first tracheostomy change (at one week post tracheotomy). Data on motor therapies were also collected. Weekly increases in growth indexes, Z scores for weight, height and weight-for-length, as well as infant’s ability to participate in motor therapies in the 4-week period prior to tracheostomy were compared to those of 4 weeks after first tracheostomy change. Data were analyzed using Chi-square, paired t-test and Wilcoxon Signed-rank test. Results: 50 infants were included in the study. Although only 15.2\% of the Infants were born small for gestational age for weight, 52.3\% had Z score \&lt;-2 and 31.8\% had Z score \&lt;-3 by the time of tracheostomy placement. This growth restriction was more severe in length with 72.7\% and 59\% of infants had Z scores \&lt;-2 and \&lt;-3 respectively. However, after tracheostomy, their mean weekly growth in weight, length, and head circumference were significantly higher (figure). Weight-to-length ratio also improved from 1.41±1.77 to 0.88±0.58, suggesting better proportional growth. In addition, infant ability to participate in developmental therapies significantly improved. Physical therapy sessions were utilized more to provide calming and assist nursing care before tracheostomy as compared to addressing developmental goals after tracheostomy (table) . Conclusion: Tracheostomy was associated with improved growth, and increased participation in age appropriate developmental activities in VLBW infants with sBPD. Our findings suggest that in infants with sBPD, tracheostomy is safe to perform and may have significant beneficial effects on growth and development.},
	number = {1\_MeetingAbstract},
	urldate = {2023-11-12},
	journal = {Pediatrics},
	author = {Zhang, Huayan and Luo, Jun and Shepard, Suzanne and Nilan, Kathleen and Harrington, Ann and Jensen, Erik and Maschhoff, Kathryn and Kirpalani, Haresh},
	month = jan,
	year = {2018},
	pages = {530},
	file = {Snapshot:/Users/shirleysong/Zotero/storage/ZUA4VSB7/Improved-Growth-and-Infant-Participation-in.html:text/html},
}

@article{truog_predicting_2014,
	title = {Predicting death or tracheostomy placement in infants with severe bronchopulmonary dysplasia},
	volume = {34},
	number = {7},
	journal = {Journal of Perinatology},
	author = {Truog, W and Grover, TR and Zhang, H and Asselin, JM and Durand, DJ and {others}},
	year = {2014},
	note = {Publisher: Nature Publishing Group},
	pages = {543--548},
}
